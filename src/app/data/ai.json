[
    {
      "question_id": 1,
      "question": "What is the difference between supervised, unsupervised, and reinforcement learning?",
      "answer": "Supervised learning uses labeled data to train models to predict outcomes (e.g., classification, regression). Unsupervised learning deals with unlabeled data and focuses on finding patterns or groupings (e.g., clustering, dimensionality reduction). Reinforcement learning involves an agent learning to make decisions by interacting with an environment to maximize cumulative reward."
    },
    {
      "question_id": 2,
      "question": "What is overfitting in machine learning and how can you prevent it?",
      "answer": "Overfitting occurs when a model learns noise and details in the training data to the extent that it negatively impacts performance on new data. Prevention strategies include using simpler models, regularization (L1/L2), data augmentation, dropout in neural networks, and cross-validation to ensure generalization."
    },
    {
      "question_id": 3,
      "question": "What is a confusion matrix, and how is it used to evaluate classification models?",
      "answer": "A confusion matrix is a table used to evaluate the performance of a classification algorithm. It shows the counts of true positives, false positives, true negatives, and false negatives. From it, you can derive metrics like accuracy, precision, recall, and F1-score to assess model performance."
    },
    {
      "question_id": 4,
      "question": "Explain the concept of gradient descent.",
      "answer": "Gradient descent is an optimization algorithm used to minimize a loss function by iteratively adjusting model parameters in the direction of the steepest descent. It calculates the gradient of the loss with respect to the parameters and updates them using a learning rate until convergence or minimal loss is achieved."
    },
    {
      "question_id": 5,
      "question": "What is the bias-variance tradeoff?",
      "answer": "The bias-variance tradeoff is the balance between a model's ability to generalize (variance) and its accuracy on the training set (bias). High bias leads to underfitting, while high variance leads to overfitting. The goal is to find a sweet spot where the model performs well on both training and unseen data."
    },
    {
      "question_id": 6,
      "question": "What are the differences between classification and regression?",
      "answer": "Classification involves predicting discrete labels (e.g., spam vs. not spam), while regression predicts continuous values (e.g., house prices). Classification uses metrics like accuracy and F1-score, while regression uses metrics like mean squared error and R² score."
    },
    {
      "question_id": 7,
      "question": "What is cross-validation and why is it important?",
      "answer": "Cross-validation is a technique for evaluating model performance by dividing the data into training and testing sets multiple times. The most common form is k-fold cross-validation. It helps ensure that the model generalizes well to unseen data and is not overfitting the training set."
    },
    {
      "question_id": 8,
      "question": "How does a decision tree algorithm work?",
      "answer": "A decision tree splits the data based on feature values to create branches that lead to predictions. It uses criteria like Gini impurity or entropy to determine the best splits. The tree is built recursively until stopping conditions are met, such as maximum depth or minimum samples per leaf."
    },
    {
      "question_id": 9,
      "question": "What is regularization in machine learning?",
      "answer": "Regularization is a technique used to reduce overfitting by adding a penalty term to the loss function. L1 regularization (Lasso) promotes sparsity by shrinking some coefficients to zero, while L2 regularization (Ridge) discourages large weights, promoting simpler models."
    },
    {
      "question_id": 10,
      "question": "What is the difference between bagging and boosting?",
      "answer": "Bagging (Bootstrap Aggregating) trains multiple models independently on random subsets of data and averages their predictions to reduce variance. Boosting trains models sequentially, where each new model focuses on correcting the errors of the previous ones, improving accuracy and reducing bias."
    },
    {
      "question_id": 11,
      "question": "What is the role of feature engineering in machine learning?",
      "answer": "Feature engineering involves creating, selecting, or transforming variables to improve the performance of machine learning models. It can involve encoding categorical variables, scaling numerical data, creating interaction terms, or deriving new features that better represent the problem domain."
    },
    {
      "question_id": 12,
      "question": "What is the difference between a generative and a discriminative model?",
      "answer": "Generative models learn the joint probability distribution of inputs and outputs (e.g., Naive Bayes, GANs), enabling them to generate new data. Discriminative models learn the conditional probability of the output given the input (e.g., logistic regression, SVM), and are typically used for classification."
    },
    {
      "question_id": 13,
      "question": "What are the main types of neural networks used in deep learning?",
      "answer": "Common types of neural networks include feedforward neural networks (basic architecture), convolutional neural networks (CNNs) for image data, recurrent neural networks (RNNs) for sequential data, and transformers for advanced language and sequence modeling tasks."
    },
    {
      "question_id": 14,
      "question": "What is transfer learning and why is it useful?",
      "answer": "Transfer learning is a technique where a model developed for one task is reused as the starting point for a model on a second task. It is useful when there is limited labeled data for the new task, and it leverages knowledge learned from large datasets to improve performance and reduce training time."
    },
    {
      "question_id": 15,
      "question": "What is the curse of dimensionality in machine learning?",
      "answer": "The curse of dimensionality refers to the exponential increase in data sparsity and computational complexity as the number of features (dimensions) grows. It can lead to overfitting, slower learning, and degraded model performance. Dimensionality reduction techniques like PCA can help mitigate this."
    },
    {
      "question_id": 16,
      "question": "How do convolutional neural networks (CNNs) process image data?",
      "answer": "CNNs process image data using convolutional layers that apply filters to capture spatial features like edges and textures. These are followed by pooling layers to reduce dimensionality and fully connected layers for classification. The local connectivity and shared weights make CNNs efficient for image tasks."
    },
    {
      "question_id": 17,
      "question": "What is an activation function, and why is it important in neural networks?",
      "answer": "An activation function introduces non-linearity into the network, allowing it to learn complex patterns. Common activation functions include ReLU, sigmoid, and tanh. Without them, neural networks would behave like linear models, limiting their learning capacity."
    },
    {
      "question_id": 18,
      "question": "What is a support vector machine (SVM) and how does it work?",
      "answer": "SVM is a supervised learning algorithm used for classification and regression. It finds the optimal hyperplane that maximizes the margin between classes. SVMs can handle non-linear data using kernel functions like polynomial or radial basis function (RBF) kernels."
    },
    {
      "question_id": 19,
      "question": "What is a ROC curve and what does it represent?",
      "answer": "A ROC (Receiver Operating Characteristic) curve is a graphical representation of a classifier's performance across various threshold settings. It plots the true positive rate against the false positive rate. The area under the curve (AUC) indicates the model’s ability to distinguish between classes."
    },
    {
      "question_id": 20,
      "question": "What are the key steps in building a machine learning pipeline?",
      "answer": "Key steps include data collection, preprocessing (cleaning, encoding, scaling), feature selection/engineering, model selection, training, validation using techniques like cross-validation, hyperparameter tuning, evaluation, and finally, deployment and monitoring in production."
    }
  ]
  